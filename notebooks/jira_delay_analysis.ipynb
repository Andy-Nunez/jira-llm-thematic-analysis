{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JIRA Delay Analysis\n",
    "\n",
    "**Approach**:\n",
    "1. Load and clean JIRA data\n",
    "2. Filter October issues delayed to November\n",
    "3. Extract and analyze comments using GPT\n",
    "4. Cluster themes and identify root causes\n",
    "5. Generate actionable management insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for custom modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JIRA CSV data\n",
    "# Update the filename to match your actual JIRA export\n",
    "data_path = 'C:\\\\Users\\\\Andy\\\\Desktop\\\\Text_Analysis\\\\data\\\\raw\\\\JiraComments_FromJql.csv'\n",
    "\n",
    "# You can obtain this file by exporting issues from JIRA using JQL queries. JIRA uses PAT tokens for authentication, with a Bearer token format. \n",
    "# If you need help generating a PAT token, or accesing the JIRA API, please send me an email at orlandoandres.nunezisaac@rochesterregional.org :)\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"‚ö†Ô∏è File not found: {data_path}\")\n",
    "    print(\"Please place your JIRA CSV export in the data/raw/ directory\")\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"‚úÖ Loaded {len(df)} records\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality check\n",
    "print(\"Data Quality Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime with timezone awareness\n",
    "# Adjust column names based on your actual JIRA export\n",
    "date_columns = ['Created', 'Updated']  # Resolved may not exist in this dataset\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        # First convert to datetime, then localize timezone\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce', utc=False)\n",
    "        # Handle timezone-aware strings\n",
    "        # Only use .dt accessor if column is datetimelike to avoid AttributeError\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            try:\n",
    "                if df[col].dt.tz is None:\n",
    "                    df[col] = df[col].dt.tz_localize('US/Eastern', ambiguous='NaT', nonexistent='NaT')\n",
    "            except Exception:\n",
    "                # If localization fails, just proceed with naive timezone\n",
    "                pass\n",
    "        print(f\"‚úÖ Converted {col} to datetime\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Column '{col}' not found\")\n",
    "\n",
    "print(\"\\nDate conversion complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the date range in the data\n",
    "print(\"Date range in data:\")\n",
    "print(f\"Earliest Created: {df['Created'].min()}\")\n",
    "print(f\"Latest Created: {df['Created'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Filter for October 2025 issues (adjust year based on actual data)\n",
    "october_start = pd.Timestamp('2025-10-01', tz='UTC').tz_convert('US/Eastern')\n",
    "october_end = pd.Timestamp('2025-10-31', tz='UTC').tz_convert('US/Eastern')\n",
    "november_start = pd.Timestamp('2025-11-01', tz='UTC').tz_convert('US/Eastern')\n",
    "\n",
    "print(f\"‚úÖ Found {len(df)} issues from October 2025\")\n",
    "print(f\"   (from {df['IssueKey'].nunique()} unique JIRA issues)\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comments from delayed issues\n",
    "# Adjust 'Comment' column name to match your data\n",
    "comment_column = 'Body'\n",
    "\n",
    "if comment_column in df.columns:\n",
    "    # Remove null comments\n",
    "    issues_with_comments = df[df[comment_column].notna()].copy()\n",
    "    print(f\"‚úÖ {len(issues_with_comments)} issues have comments\")\n",
    "    \n",
    "    # Sample of comments\n",
    "    print(\"\\nSample comments:\")\n",
    "    for idx, comment in issues_with_comments[comment_column].head(3).items():\n",
    "        print(f\"\\n--- Issue {idx} ---\")\n",
    "        print(comment[:200] + \"...\" if len(str(comment)) > 200 else comment)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Column '{comment_column}' not found. Available columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. Data Aggregation Strategy (GROUP BY ISSUEKEY)\n",
    "\n",
    "**Methodology Section 1.2**: Group comments by IssueKey to reduce API calls from 241 to 39.\n",
    "\n",
    "This provides:\n",
    "1. Complete conversational context for the LLM\n",
    "2. More accurate thematic inference\n",
    "3. Significant cost reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Group comments by IssueKey and calculate metrics\n",
    "issue_groups = []\n",
    "\n",
    "for issue_key, group in issues_with_comments.groupby('IssueKey'):\n",
    "    # Combine all comments for this issue\n",
    "    combined_comments = \"\\n---\\n\".join(group['Body'].tolist())\n",
    "    \n",
    "    # Calculate time metrics\n",
    "    first_comment_date = group['Created'].min()\n",
    "    last_comment_date = group['Created'].max()\n",
    "    days_active = (last_comment_date - first_comment_date).days\n",
    "    \n",
    "    # Calculate other metrics\n",
    "    comment_count = len(group)\n",
    "    unique_authors = group['Author'].nunique()\n",
    "    authors_list = ', '.join(group['Author'].unique()[:3])\n",
    "    \n",
    "    issue_groups.append({\n",
    "        'IssueKey': issue_key,\n",
    "        'combined_comments': combined_comments,\n",
    "        'comment_count': comment_count,\n",
    "        'unique_authors': unique_authors,\n",
    "        'authors_sample': authors_list,\n",
    "        'first_comment_date': first_comment_date,\n",
    "        'last_comment_date': last_comment_date,\n",
    "        'days_active': days_active\n",
    "    })\n",
    "\n",
    "issues_df = pd.DataFrame(issue_groups)\n",
    "\n",
    "print(f\"‚úÖ Grouped into {len(issues_df)} IssueKeys for analysis\")\n",
    "print(f\"‚úÖ API calls reduced from {len(issues_with_comments)} to {len(issues_df)}\")\n",
    "print(f\"\\nSample grouped data:\")\n",
    "display(issues_df[['IssueKey', 'comment_count', 'unique_authors', 'days_active']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. LLM-Based Theme Extraction\n",
    "\n",
    "**Methodology Section 1.3**: Analyze each aggregated issue conversation with GPT-4.\n",
    "\n",
    "Extract:\n",
    "- Primary delay factors (thematic categories)\n",
    "- Sentiment indicators\n",
    "- Root cause attribution\n",
    "- Structured output in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom LLM analyzer\n",
    "from llm_analyzer import DelayThemeAnalyzer\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = DelayThemeAnalyzer()\n",
    "\n",
    "print(\"‚úÖ LLM analyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run LLM analysis on GROUPED comments (much more efficient!)\n",
    "delay_themes_grouped = []\n",
    "\n",
    "print(f\"Analyzing {len(issues_df)} IssueKeys...\")\n",
    "print(f\"Estimated time: ~{len(issues_df) * 3 / 60:.1f} minutes\\n\")\n",
    "\n",
    "for idx, row in tqdm(issues_df.iterrows(), total=len(issues_df), desc=\"Extracting themes\"):\n",
    "    issue_key = row['IssueKey']\n",
    "    combined_text = row['combined_comments']\n",
    "    \n",
    "    # Get delay theme from GPT (1 call for all comments in this issue)\n",
    "    theme_result = analyzer.extract_delay_theme(combined_text, issue_key)\n",
    "    delay_themes_grouped.append(theme_result)\n",
    "\n",
    "# Convert to DataFrame\n",
    "themes_grouped_df = pd.DataFrame(delay_themes_grouped)\n",
    "\n",
    "print(f\"\\n‚úÖ Analyzed {len(themes_grouped_df)} IssueKeys\")\n",
    "print(f\"‚úÖ Extracted {themes_grouped_df['theme'].nunique()} unique themes\")\n",
    "display(themes_grouped_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge theme results with issue metrics\n",
    "issues_with_themes = issues_df.merge(\n",
    "    themes_grouped_df[['issue_key', 'theme', 'sentiment', 'reasoning']],\n",
    "    left_on='IssueKey',\n",
    "    right_on='issue_key',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "issues_with_themes = issues_with_themes.drop(columns=['issue_key'])\n",
    "\n",
    "print(f\"‚úÖ Merged theme data with issue metrics\")\n",
    "display(issues_with_themes[[\n",
    "    'IssueKey', 'theme', 'sentiment', 'comment_count',\n",
    "    'days_active', 'unique_authors'\n",
    "]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Post-Processing and Theme Clustering\n",
    "\n",
    "**Methodology Section 1.4**: Use K-means clustering to group similar themes into 5 major clusters.\n",
    "\n",
    "This addresses semantic redundancy and consolidates thematically similar delay factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text for clustering\n",
    "issues_with_themes['text_for_clustering'] = (\n",
    "    issues_with_themes['theme'].fillna('Unknown') + ' ' +\n",
    "    issues_with_themes['reasoning'].fillna('')\n",
    ")\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(issues_with_themes['text_for_clustering'])\n",
    "\n",
    "print(f\"‚úÖ Vectorized {X.shape[0]} issues into {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering\n",
    "n_clusters = 5\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "issues_with_themes['cluster'] = cluster_labels\n",
    "\n",
    "print(f\"‚úÖ Clustered themes into {n_clusters} groups\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(issues_with_themes['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cluster names\n",
    "cluster_names = {}\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_issues = issues_with_themes[issues_with_themes['cluster'] == cluster_id]\n",
    "    top_theme = cluster_issues['theme'].value_counts().index[0]\n",
    "    count = len(cluster_issues)\n",
    "    \n",
    "    cluster_names[cluster_id] = f\"Cluster {cluster_id + 1}: {top_theme} ({count} issues)\"\n",
    "    \n",
    "    print(f\"\\n{cluster_names[cluster_id]}\")\n",
    "    print(f\"  Top themes:\")\n",
    "    for theme, cnt in cluster_issues['theme'].value_counts().head(3).items():\n",
    "        print(f\"    - {theme}: {cnt}\")\n",
    "\n",
    "issues_with_themes['cluster_name'] = issues_with_themes['cluster'].map(cluster_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. COMPREHENSIVE REPORTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive report\n",
    "comprehensive_report = issues_with_themes[[\n",
    "    'IssueKey',\n",
    "    'cluster',\n",
    "    'cluster_name',\n",
    "    'theme',\n",
    "    'sentiment',\n",
    "    'days_active',\n",
    "    'comment_count',\n",
    "    'unique_authors',\n",
    "    'authors_sample',\n",
    "    'first_comment_date',\n",
    "    'last_comment_date',\n",
    "    'reasoning'\n",
    "]].copy()\n",
    "\n",
    "comprehensive_report = comprehensive_report.sort_values(\n",
    "    ['cluster', 'days_active'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "comprehensive_report.to_csv('../reports/comprehensive_delay_analysis.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Saved comprehensive report\")\n",
    "print(f\"\\nReport preview:\")\n",
    "display(comprehensive_report.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 Clusters Summary\n",
    "cluster_summary = []\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_data = issues_with_themes[issues_with_themes['cluster'] == cluster_id]\n",
    "    \n",
    "    cluster_summary.append({\n",
    "        'Rank': cluster_id + 1,\n",
    "        'Cluster_Name': cluster_names[cluster_id],\n",
    "        'Issue_Count': len(cluster_data),\n",
    "        'Percentage': round(len(cluster_data) / len(issues_with_themes) * 100, 1),\n",
    "        'Avg_Days_Active': round(cluster_data['days_active'].mean(), 1),\n",
    "        'Avg_Comments': round(cluster_data['comment_count'].mean(), 1),\n",
    "        'Negative_Sentiment_Count': (cluster_data['sentiment'] == 'negative').sum(),\n",
    "        'Top_Theme': cluster_data['theme'].value_counts().index[0],\n",
    "        'Sample_IssueKeys': ', '.join(cluster_data['IssueKey'].head(5).tolist())\n",
    "    })\n",
    "\n",
    "cluster_summary_df = pd.DataFrame(cluster_summary)\n",
    "cluster_summary_df.to_csv('../reports/top_5_cluster_summary.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Saved top 5 cluster summary\")\n",
    "display(cluster_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EXECUTIVE SUMMARY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RRH SUMMARY: JIRA DELAY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAnalysis Period: OCT-NOV 2025\")\n",
    "print(f\"Dataset: {len(issues_with_comments)} comments from {len(issues_with_themes)} unique issues\")\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"\\nTOP 5 DELAY THEME CLUSTERS:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for idx, row in cluster_summary_df.iterrows():\n",
    "    print(f\"\\n{row['Rank']}. {row['Cluster_Name']}\")\n",
    "    print(f\"   Issues: {row['Issue_Count']} ({row['Percentage']}% of total)\")\n",
    "    print(f\"   Avg Duration: {row['Avg_Days_Active']} days active\")\n",
    "    print(f\"   Avg Comments: {row['Avg_Comments']} per issue\")\n",
    "    print(f\"   Negative Sentiment: {row['Negative_Sentiment_Count']} issues\")\n",
    "    print(f\"   Dominant Theme: {row['Top_Theme']}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"\\n1. Most Common Delay Cluster: {cluster_summary_df.iloc[0]['Cluster_Name']}\")\n",
    "print(f\"   ‚Üí Affects {cluster_summary_df.iloc[0]['Issue_Count']} issues\")\n",
    "\n",
    "longest_cluster = cluster_summary_df.loc[cluster_summary_df['Avg_Days_Active'].idxmax()]\n",
    "print(f\"\\n2. Longest Average Delay: {longest_cluster['Cluster_Name']}\")\n",
    "print(f\"   ‚Üí Average {longest_cluster['Avg_Days_Active']} days active\")\n",
    "\n",
    "most_negative = cluster_summary_df.loc[cluster_summary_df['Negative_Sentiment_Count'].idxmax()]\n",
    "print(f\"\\n3. Most Frustrating Cluster: {most_negative['Cluster_Name']}\")\n",
    "print(f\"   ‚Üí {most_negative['Negative_Sentiment_Count']} issues with negative sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "## 9. Visualizations and Analysis\n",
    "\n",
    "# A lot of these were experimental, so please use with that in mind. Some of them are broke, and others not meaningful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_counts = issues_with_themes['cluster'].value_counts().sort_index()\n",
    "\n",
    "sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis')\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Number of Issues', fontsize=12)\n",
    "plt.title('Distribution of Issues Across 5 Theme Clusters', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(n_clusters), [f\"C{i+1}\" for i in range(n_clusters)])\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/cluster_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved cluster visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    X_pca[:, 0], X_pca[:, 1],\n",
    "    c=cluster_labels,\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.6,\n",
    "    edgecolors='black'\n",
    ")\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "plt.title('Theme Clusters Visualization (PCA)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/cluster_pca_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved PCA visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cluster Distribution Pie Chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "cluster_counts = issues_with_themes['cluster'].value_counts().sort_index()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "plt.pie(cluster_counts.values, \n",
    "        labels=[f\"C{i+1}\" for i in range(n_clusters)],\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors,\n",
    "        startangle=90,\n",
    "        textprops={'fontsize': 11, 'weight': 'bold'})\n",
    "\n",
    "plt.title('Issue Distribution Across 5 Theme Clusters', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add legend with cluster names\n",
    "legend_labels = [cluster_names[i] for i in range(n_clusters)]\n",
    "plt.legend(legend_labels, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/viz_01_cluster_pie.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: viz_01_cluster_pie.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sentiment Distribution Pie Chart\n",
    "plt.figure(figsize=(10, 7))\n",
    "sentiment_counts = issues_with_themes['sentiment'].value_counts()\n",
    "colors_sentiment = {'negative': '#e74c3c', 'neutral': '#95a5a6', 'positive': '#2ecc71'}\n",
    "pie_colors = [colors_sentiment.get(s, '#3498db') for s in sentiment_counts.index]\n",
    "\n",
    "plt.pie(sentiment_counts.values,\n",
    "        labels=[s.capitalize() for s in sentiment_counts.index],\n",
    "        autopct='%1.1f%%',\n",
    "        colors=pie_colors,\n",
    "        startangle=140,\n",
    "        textprops={'fontsize': 12, 'weight': 'bold'})\n",
    "\n",
    "plt.title('Overall Sentiment Distribution\\nAcross All Delayed Issues', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/viz_02_sentiment_pie.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: viz_02_sentiment_pie.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Days Active Distribution by Cluster (Box Plot)\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Prepare data\n",
    "cluster_days_data = [issues_with_themes[issues_with_themes['cluster'] == i]['days_active'].values \n",
    "                     for i in range(n_clusters)]\n",
    "\n",
    "cluster_labels = [cluster_names[i].split(': ', 1)[1] if ': ' in cluster_names[i] else cluster_names[i] \n",
    "                  for i in range(n_clusters)]\n",
    "\n",
    "bp = plt.boxplot(cluster_days_data, \n",
    "                 labels=cluster_labels, \n",
    "                 patch_artist=True,\n",
    "                 showmeans=True,\n",
    "                 meanprops=dict(marker='D', markerfacecolor='red', markersize=8))\n",
    "\n",
    "# Color boxes\n",
    "for patch, color in zip(bp['boxes'], plt.cm.viridis(np.linspace(0, 1, n_clusters))):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.xlabel('Cluster', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Days Active (First to Last Comment)', fontsize=12, fontweight='bold')\n",
    "plt.title('Issue Duration Distribution by Cluster\\n(Red diamond = mean, line = median)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/viz_03_days_active_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: viz_03_days_active_boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Comment Count Distribution by Cluster (Violin Plot)\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Prepare data for violin plot\n",
    "positions = range(n_clusters)\n",
    "violin_data = [issues_with_themes[issues_with_themes['cluster'] == i]['comment_count'].values \n",
    "               for i in range(n_clusters)]\n",
    "\n",
    "parts = plt.violinplot(violin_data, positions=positions, showmeans=True, showmedians=True)\n",
    "\n",
    "# Color violins\n",
    "for i, pc in enumerate(parts['bodies']):\n",
    "    pc.set_facecolor(plt.cm.viridis(i / n_clusters))\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "plt.xlabel('Cluster', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Comments per Issue', fontsize=12, fontweight='bold')\n",
    "plt.title('Comment Activity Distribution by Cluster\\n(Violin plot shows distribution density)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xticks(positions, [f\"C{i+1}\" for i in range(n_clusters)])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/viz_04_comments_violin.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: viz_04_comments_violin.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Timeline: Issue Activity Over Time\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Handle timezone-aware datetime (convert to timezone-naive for plotting)\n",
    "issues_with_themes['first_comment_date'] = pd.to_datetime(\n",
    "    issues_with_themes['first_comment_date'], \n",
    "    utc=True\n",
    ").dt.tz_localize(None)  # Remove timezone info for plotting\n",
    "\n",
    "# Create timeline data\n",
    "issues_with_themes_sorted = issues_with_themes.sort_values('first_comment_date')\n",
    "\n",
    "# Count issues by date\n",
    "date_counts = issues_with_themes_sorted.groupby(\n",
    "    issues_with_themes_sorted['first_comment_date'].dt.date\n",
    ").size()\n",
    "\n",
    "plt.plot(date_counts.index, date_counts.values, \n",
    "         marker='o', linewidth=2, markersize=8, color='#3498db')\n",
    "\n",
    "plt.fill_between(date_counts.index, date_counts.values, alpha=0.3, color='#3498db')\n",
    "\n",
    "plt.xlabel('Date', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Issues Starting', fontsize=12, fontweight='bold')\n",
    "plt.title('Issue Activity Timeline\\n(When did issues start getting comments?)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/viz_06_timeline.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: viz_06_timeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Top 10 Most Active Authors\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Count comments by author from original data\n",
    "author_counts = issues_with_comments['Author'].value_counts().head(10)\n",
    "\n",
    "colors = plt.cm.plasma(np.linspace(0, 0.8, len(author_counts)))\n",
    "bars = plt.barh(range(len(author_counts)), author_counts.values, color=colors)\n",
    "\n",
    "plt.yticks(range(len(author_counts)), author_counts.index)\n",
    "plt.xlabel('Number of Comments', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Author', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 10 Most Active Comment Authors', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, value) in enumerate(zip(bars, author_counts.values)):\n",
    "    plt.text(value + 0.5, i, str(value), va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/viz_07_top_authors.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: viz_07_top_authors.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Bubble Chart: Days Active vs Comment Count (sized by authors)\n",
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "# Create bubble chart\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_data = issues_with_themes[issues_with_themes['cluster'] == cluster_id]\n",
    "    \n",
    "    plt.scatter(\n",
    "        cluster_data['days_active'],\n",
    "        cluster_data['comment_count'],\n",
    "        s=cluster_data['unique_authors'] * 100,  # Size by unique authors\n",
    "        alpha=0.6,\n",
    "        c=[plt.cm.viridis(cluster_id / n_clusters)] * len(cluster_data),\n",
    "        label=f\"Cluster {cluster_id + 1}\",\n",
    "        edgecolors='black',\n",
    "        linewidth=1\n",
    "    )\n",
    "\n",
    "plt.xlabel('Days Active (First to Last Comment)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Comments', fontsize=12, fontweight='bold')\n",
    "plt.title('Issue Activity Bubble Chart\\n(Bubble size = number of unique authors)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/viz_08_bubble_chart.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: viz_08_bubble_chart.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = [cluster_names[i].split(': ', 1)[1] if ': ' in cluster_names[i] else cluster_names[i] \n",
    "                  for i in range(n_clusters)]\n",
    "\n",
    "# 10. Cluster Metrics Comparison (Multi-metric bar chart)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "\n",
    "\n",
    "# Metric 1: Average Days Active\n",
    "ax1 = axes[0, 0]\n",
    "avg_days = [issues_with_themes[issues_with_themes['cluster'] == i]['days_active'].mean() \n",
    "            for i in range(n_clusters)]\n",
    "bars1 = ax1.bar(range(n_clusters), avg_days, color=plt.cm.viridis(np.linspace(0, 1, n_clusters)))\n",
    "ax1.set_xlabel('Cluster', fontweight='bold')\n",
    "ax1.set_ylabel('Average Days Active', fontweight='bold')\n",
    "ax1.set_title('Average Duration by Cluster', fontweight='bold')\n",
    "ax1.set_xticks(range(n_clusters))\n",
    "ax1.set_xticklabels([f\"C{i+1}\" for i in range(n_clusters)])\n",
    "for bar, val in zip(bars1, avg_days):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, val + 0.2, f'{val:.1f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Metric 2: Average Comment Count\n",
    "ax2 = axes[0, 1]\n",
    "avg_comments = [issues_with_themes[issues_with_themes['cluster'] == i]['comment_count'].mean() \n",
    "                for i in range(n_clusters)]\n",
    "bars2 = ax2.bar(range(n_clusters), avg_comments, color=plt.cm.plasma(np.linspace(0, 1, n_clusters)))\n",
    "ax2.set_xlabel('Cluster', fontweight='bold')\n",
    "ax2.set_ylabel('Average Comments', fontweight='bold')\n",
    "ax2.set_title('Average Comment Count by Cluster', fontweight='bold')\n",
    "ax2.set_xticks(range(n_clusters))\n",
    "ax2.set_xticklabels([f\"C{i+1}\" for i in range(n_clusters)])\n",
    "for bar, val in zip(bars2, avg_comments):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, val + 0.1, f'{val:.1f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Metric 3: Issue Count\n",
    "ax3 = axes[1, 0]\n",
    "issue_counts = [len(issues_with_themes[issues_with_themes['cluster'] == i]) \n",
    "                for i in range(n_clusters)]\n",
    "bars3 = ax3.bar(range(n_clusters), issue_counts, color=plt.cm.cool(np.linspace(0, 1, n_clusters)))\n",
    "ax3.set_xlabel('Cluster', fontweight='bold')\n",
    "ax3.set_ylabel('Number of Issues', fontweight='bold')\n",
    "ax3.set_title('Issue Count by Cluster', fontweight='bold')\n",
    "ax3.set_xticks(range(n_clusters))\n",
    "ax3.set_xticklabels([f\"C{i+1}\" for i in range(n_clusters)])\n",
    "for bar, val in zip(bars3, issue_counts):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, val + 0.2, str(val), \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Metric 4: Negative Sentiment Count\n",
    "ax4 = axes[1, 1]\n",
    "neg_counts = [(issues_with_themes[issues_with_themes['cluster'] == i]['sentiment'] == 'negative').sum() \n",
    "              for i in range(n_clusters)]\n",
    "bars4 = ax4.bar(range(n_clusters), neg_counts, color='#e74c3c', alpha=0.7)\n",
    "ax4.set_xlabel('Cluster', fontweight='bold')\n",
    "ax4.set_ylabel('Negative Sentiment Count', fontweight='bold')\n",
    "ax4.set_title('Issues with Negative Sentiment by Cluster', fontweight='bold')\n",
    "ax4.set_xticks(range(n_clusters))\n",
    "ax4.set_xticklabels([f\"C{i+1}\" for i in range(n_clusters)])\n",
    "for bar, val in zip(bars4, neg_counts):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, val + 0.1, str(val), \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Comprehensive Cluster Metrics Comparison', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/viz_10_cluster_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: viz_10_cluster_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Cluster Metrics Comparison (Multi-metric bar chart)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Create short cluster labels (remove \"Cluster X: \" prefix)\n",
    "cluster_labels_short = [cluster_names[i].split(': ', 1)[1] if ': ' in cluster_names[i] else cluster_names[i] \n",
    "                        for i in range(n_clusters)]\n",
    "\n",
    "# Metric 1: Average Days Active\n",
    "ax1 = axes[0, 0]\n",
    "avg_days = [issues_with_themes[issues_with_themes['cluster'] == i]['days_active'].mean() \n",
    "            for i in range(n_clusters)]\n",
    "bars1 = ax1.bar(range(n_clusters), avg_days, color=plt.cm.plasma(np.linspace(0, 1, n_clusters)))\n",
    "ax1.set_xlabel('Cluster', fontweight='bold')\n",
    "ax1.set_ylabel('Average Days Active', fontweight='bold')\n",
    "ax1.set_title('Average Duration by Cluster', fontweight='bold')\n",
    "ax1.set_xticks(range(n_clusters))\n",
    "ax1.set_xticklabels(cluster_labels_short, rotation=45, ha='right', fontsize=9)\n",
    "for bar, val in zip(bars1, avg_days):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, val + 0.2, f'{val:.1f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Metric 2: Average Comment Count\n",
    "ax2 = axes[0, 1]\n",
    "avg_comments = [issues_with_themes[issues_with_themes['cluster'] == i]['comment_count'].mean() \n",
    "                for i in range(n_clusters)]\n",
    "bars2 = ax2.bar(range(n_clusters), avg_comments, color=plt.cm.viridis(np.linspace(0, 1, n_clusters)))\n",
    "ax2.set_xlabel('Cluster', fontweight='bold')\n",
    "ax2.set_ylabel('Average Comments', fontweight='bold')\n",
    "ax2.set_title('Average Comment Count by Cluster', fontweight='bold')\n",
    "ax2.set_xticks(range(n_clusters))\n",
    "ax2.set_xticklabels(cluster_labels_short, rotation=45, ha='right', fontsize=9)\n",
    "for bar, val in zip(bars2, avg_comments):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, val + 0.1, f'{val:.1f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Metric 3: Issue Count\n",
    "ax3 = axes[1, 0]\n",
    "issue_counts = [len(issues_with_themes[issues_with_themes['cluster'] == i]) \n",
    "                for i in range(n_clusters)]\n",
    "bars3 = ax3.bar(range(n_clusters), issue_counts, color=plt.cm.cool(np.linspace(0, 1, n_clusters)))\n",
    "ax3.set_xlabel('Cluster', fontweight='bold')\n",
    "ax3.set_ylabel('Number of Issues', fontweight='bold')\n",
    "ax3.set_title('Issue Count by Cluster', fontweight='bold')\n",
    "ax3.set_xticks(range(n_clusters))\n",
    "ax3.set_xticklabels(cluster_labels_short, rotation=45, ha='right', fontsize=9)\n",
    "for bar, val in zip(bars3, issue_counts):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, val + 0.2, str(val), \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Metric 4: Negative Sentiment Count\n",
    "ax4 = axes[1, 1]\n",
    "neg_counts = [(issues_with_themes[issues_with_themes['cluster'] == i]['sentiment'] == 'negative').sum() \n",
    "              for i in range(n_clusters)]\n",
    "bars4 = ax4.bar(range(n_clusters), neg_counts, color='#e74c3c', alpha=0.7)\n",
    "ax4.set_xlabel('Cluster', fontweight='bold')\n",
    "ax4.set_ylabel('Negative Sentiment Count', fontweight='bold')\n",
    "ax4.set_title('Issues with Negative Sentiment by Cluster', fontweight='bold')\n",
    "ax4.set_xticks(range(n_clusters))\n",
    "ax4.set_xticklabels(cluster_labels_short, rotation=45, ha='right', fontsize=9)\n",
    "for bar, val in zip(bars4, neg_counts):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, val + 0.1, str(val), \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Comprehensive Cluster Metrics Comparison', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/viz_10_cluster_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: viz_10_cluster_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed reasoning analysis with wrapped text\n",
    "import textwrap\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CLUSTER REASONING ANALYSIS - DETAILED VIEW\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_data = issues_with_themes[issues_with_themes['cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"CLUSTER {cluster_id + 1}: {cluster_names[cluster_id]}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"üìä Metrics: {len(cluster_data)} issues | Avg {cluster_data['days_active'].mean():.1f} days active | {cluster_data['comment_count'].mean():.1f} comments\")\n",
    "    print()\n",
    "    \n",
    "    # Show all issues in this cluster with reasoning\n",
    "    for idx, row in cluster_data.iterrows():\n",
    "        print(f\"‚îå‚îÄ {row['IssueKey']} ‚îÄ {row['theme']}\")\n",
    "        print(f\"‚îÇ  üìÖ Days Active: {row['days_active']} | üí¨ Comments: {row['comment_count']} | üë• Authors: {row['unique_authors']}\")\n",
    "        print(f\"‚îÇ  üòä Sentiment: {row['sentiment'].upper()}\")\n",
    "        \n",
    "        # Wrap the reasoning text to 90 characters\n",
    "        wrapped_reasoning = textwrap.fill(row['reasoning'], width=90, initial_indent='‚îÇ  üìù ', subsequent_indent='‚îÇ     ')\n",
    "        print(wrapped_reasoning)\n",
    "        print(\"‚îî\" + \"‚îÄ\" * 98)\n",
    "        print()\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"‚úÖ Reasoning analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10-header",
   "metadata": {},
   "source": [
    "## 10. Export Results\n",
    "\n",
    "Generate final PDF report with cluster reasoning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install reportlab\n",
    "\n",
    "# Export reasoning analysis to PDF\n",
    "from reportlab.lib.pagesizes import letter, A4\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, Table, TableStyle\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.enums import TA_LEFT, TA_CENTER\n",
    "import textwrap\n",
    "\n",
    "# Create PDF\n",
    "pdf_filename = '../reports/cluster_reasoning_analysis.pdf'\n",
    "doc = SimpleDocTemplate(pdf_filename, pagesize=letter, topMargin=0.75*inch, bottomMargin=0.75*inch)\n",
    "story = []\n",
    "\n",
    "# Styles\n",
    "styles = getSampleStyleSheet()\n",
    "title_style = ParagraphStyle(\n",
    "    'CustomTitle',\n",
    "    parent=styles['Heading1'],\n",
    "    fontSize=18,\n",
    "    textColor=colors.HexColor('#2c3e50'),\n",
    "    spaceAfter=30,\n",
    "    alignment=TA_CENTER\n",
    ")\n",
    "heading_style = ParagraphStyle(\n",
    "    'CustomHeading',\n",
    "    parent=styles['Heading2'],\n",
    "    fontSize=14,\n",
    "    textColor=colors.HexColor('#34495e'),\n",
    "    spaceAfter=12,\n",
    "    spaceBefore=12\n",
    ")\n",
    "normal_style = styles['Normal']\n",
    "\n",
    "# Title\n",
    "story.append(Paragraph(\"JIRA Delay Analysis - Cluster Reasoning Report\", title_style))\n",
    "story.append(Paragraph(f\"Analysis Period: October 2025\", normal_style))\n",
    "story.append(Paragraph(f\"Total Issues Analyzed: {len(issues_with_themes)}\", normal_style))\n",
    "story.append(Spacer(1, 0.3*inch))\n",
    "\n",
    "# Add each cluster\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_data = issues_with_themes[issues_with_themes['cluster'] == cluster_id]\n",
    "    \n",
    "    # Cluster header\n",
    "    story.append(Paragraph(f\"{cluster_id + 1}: {cluster_names[cluster_id]}\", heading_style))\n",
    "    \n",
    "    # Metrics summary\n",
    "    metrics_text = f\"<b>Metrics:</b> {len(cluster_data)} issues | Avg {cluster_data['days_active'].mean():.1f} days active | {cluster_data['comment_count'].mean():.1f} comments\"\n",
    "    story.append(Paragraph(metrics_text, normal_style))\n",
    "    story.append(Spacer(1, 0.15*inch))\n",
    "    \n",
    "    # Create table for issues\n",
    "    table_data = [['IssueKey', 'Theme', 'Days', 'Comments', 'Sentiment', 'Reasoning']]\n",
    "    \n",
    "    for idx, row in cluster_data.iterrows():\n",
    "        # Wrap reasoning text\n",
    "        wrapped_reasoning = textwrap.fill(row['reasoning'], width=40)\n",
    "        \n",
    "        table_data.append([\n",
    "            row['IssueKey'],\n",
    "            textwrap.fill(row['theme'], width=20),\n",
    "            str(row['days_active']),\n",
    "            str(row['comment_count']),\n",
    "            row['sentiment'],\n",
    "            wrapped_reasoning\n",
    "        ])\n",
    "    \n",
    "    # Create table\n",
    "    t = Table(table_data, colWidths=[1*inch, 1.5*inch, 0.6*inch, 0.7*inch, 0.7*inch, 2.5*inch])\n",
    "    t.setStyle(TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#3498db')),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 10),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "        ('VALIGN', (0, 0), (-1, -1), 'TOP'),\n",
    "        ('FONTSIZE', (0, 1), (-1, -1), 8),\n",
    "    ]))\n",
    "    \n",
    "    story.append(t)\n",
    "    story.append(PageBreak())\n",
    "\n",
    "# Build PDF\n",
    "doc.build(story)\n",
    "print(f\"‚úÖ PDF exported to: {pdf_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
