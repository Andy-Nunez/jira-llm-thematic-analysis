{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b78824",
   "metadata": {},
   "source": [
    "# Jira Comments â€“ LLM-Assisted Thematic Analysis\n",
    "\n",
    "1. **Load and clean** Jira comments exported from your PowerShell script.  \n",
    "2. **Normalize** multiple comments per SWIFT (issue key) while keeping the link between each comment and its issue.  \n",
    "3. **De-identify** and prepare the comment text for upload to an LLM (e.g. Copilot / GPT) for theme discovery and labeling.  \n",
    "4. (Optional) **Join back** LLM-generated labels with the comments for further analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36655cbe",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 â€“ Imports and basic config\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "# OpenAI import for automated theme analysis\n",
    "from openai import OpenAI\n",
    "\n",
    "def analyze_themes_automated(comments_df, num_themes=10):\n",
    "    \"\"\"Automated theme discovery using LLM API\"\"\"\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    # Prepare prompt\n",
    "    prompt = f\"\"\"Analyze these Jira comments about project delays.\n",
    "    \n",
    "{comments_df['text_clean'].to_string()}\n",
    "\n",
    "Identify {num_themes} high-level themes. Return JSON:\n",
    "{{\n",
    "  \"themes\": [\n",
    "    {{\n",
    "      \"theme\": \"Theme name\",\n",
    "      \"description\": \"One sentence description\",\n",
    "      \"examples\": [\"phrase 1\", \"phrase 2\", \"phrase 3\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee1b00",
   "metadata": {},
   "source": [
    "## 2. Load the Jira comments CSV\n",
    "\n",
    "Update the `csv_path` below to point to your exported `JiraComments_FromJql.csv` (or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f752fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 â€“ Load CSV\n",
    "\n",
    "csv_path = r\"C:\\Users\\Andy\\Desktop\\Text_Analysis\\JiraComments_FromJql.csv\"  # <-- change this to your actual path\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"CSV not found at: {csv_path}\\nPlease update csv_path to the correct location.\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", list(df.columns))\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71bad8",
   "metadata": {},
   "source": [
    "## 3. Normalize structure: one row = one comment\n",
    "\n",
    "Here we:\n",
    "\n",
    "- Identify the likely **issue key column** (e.g. `IssueKey`).  \n",
    "- Identify the **comment body column** (e.g. `Body`).  \n",
    "- Keep those plus any IDs that help us track comments later (e.g. `CommentId`).  \n",
    "- Ensure that each row is a single comment with its associated SWIFT / Jira issue key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 â€“ Normalize and select key columns\n",
    "\n",
    "possible_issue_cols = [\"IssueKey\", \"issueKey\", \"Key\", \"key\"]\n",
    "possible_body_cols  = [\"Body\", \"body\", \"Comment\", \"comment\", \"Text\", \"text\"]\n",
    "possible_id_cols    = [\"CommentId\", \"commentId\", \"Id\", \"id\"]\n",
    "possible_time_cols  = [\"Created\", \"created\", \"Timestamp\", \"timestamp\", \"Updated\", \"updated\"]\n",
    "\n",
    "issue_col = next((c for c in possible_issue_cols if c in df.columns), None)\n",
    "body_col  = next((c for c in possible_body_cols  if c in df.columns), None)\n",
    "id_col    = next((c for c in possible_id_cols    if c in df.columns), None)\n",
    "time_col  = next((c for c in possible_time_cols  if c in df.columns), None)\n",
    "\n",
    "if issue_col is None or body_col is None:\n",
    "    raise ValueError(\n",
    "        f\"Could not find issue or body column.\\n\"\n",
    "        f\"Available columns: {list(df.columns)}\\n\"\n",
    "        f\"Expected something like {possible_issue_cols} and {possible_body_cols}\"\n",
    "    )\n",
    "\n",
    "print(f\"Using issue column : {issue_col}\")\n",
    "print(f\"Using body column  : {body_col}\")\n",
    "print(f\"Using ID column    : {id_col if id_col else '(none â€“ will create one)'}\")\n",
    "print(f\"Using time column  : {time_col if time_col else '(none found)'}\")\n",
    "\n",
    "# Build a normalized DataFrame\n",
    "cols_to_keep = [issue_col, body_col]\n",
    "if id_col:\n",
    "    cols_to_keep.append(id_col)\n",
    "if time_col:\n",
    "    cols_to_keep.append(time_col)\n",
    "\n",
    "data = df[cols_to_keep].copy()\n",
    "data.rename(columns={issue_col: \"IssueKey\", body_col: \"Body\"}, inplace=True)\n",
    "if time_col:\n",
    "    data.rename(columns={time_col: \"Timestamp\"}, inplace=True)\n",
    "\n",
    "# Create a synthetic comment_id if none exists\n",
    "if id_col is None:\n",
    "    data.insert(0, \"CommentSeq\", np.arange(1, len(data) + 1))\n",
    "else:\n",
    "    data.rename(columns={id_col: \"CommentId\"}, inplace=True)\n",
    "    if \"CommentId\" not in data.columns:\n",
    "        data.insert(0, \"CommentSeq\", np.arange(1, len(data) + 1))\n",
    "\n",
    "# Ensure Body is string and non-empty\n",
    "data[\"Body\"] = data[\"Body\"].astype(str).str.strip()\n",
    "data = data[data[\"Body\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# If we have a timestamp, parse and sort by IssueKey + Timestamp\n",
    "if \"Timestamp\" in data.columns:\n",
    "    data[\"Timestamp\"] = pd.to_datetime(data[\"Timestamp\"], errors=\"coerce\")\n",
    "    data = data.sort_values([\"IssueKey\", \"Timestamp\"], ascending=[True, True])\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nAfter normalization:\")\n",
    "print(\"Rows (comments):\", len(data))\n",
    "print(\"Distinct SWIFT / issue keys:\", data[\"IssueKey\"].nunique())\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf75a41",
   "metadata": {},
   "source": [
    "## 4. De-identification & text cleaning\n",
    "\n",
    "We will:\n",
    "\n",
    "- Remove Jira user mentions like `[~USER]`.  \n",
    "- Remove obvious CRQ numbers (`CRQ1234567`).  \n",
    "- Remove email addresses.  \n",
    "- Optionally remove issue keys themselves from the comment text.  \n",
    "- Keep a clean text column suitable for sending to an LLM.\n",
    "\n",
    "We **do not** touch the `IssueKey` column itself so we can always trace a comment back to its SWIFT later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 â€“ De-identification and cleaning\n",
    "\n",
    "mention_pattern    = re.compile(r\"\\[~[^\\]]+\\]\")         # [~username]\n",
    "crq_pattern        = re.compile(r\"CRQ\\d+\", re.IGNORECASE)\n",
    "email_pattern      = re.compile(r\"[\\w\\.\\-]+@[\\w\\.-]+\")\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_and_deidentify(row):\n",
    "    text = str(row[\"Body\"])\n",
    "\n",
    "    # Remove Jira mentions like [~SMULLIN]\n",
    "    text = mention_pattern.sub(\" \", text)\n",
    "\n",
    "    # Remove CRQ-style change request numbers\n",
    "    text = crq_pattern.sub(\" \", text)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = email_pattern.sub(\" \", text)\n",
    "\n",
    "    # Remove the explicit issue key text if it appears inside the body\n",
    "    issue_key = str(row[\"IssueKey\"])\n",
    "    if issue_key and issue_key in text:\n",
    "        text = text.replace(issue_key, \" \")\n",
    "\n",
    "    # Remove extra non-informative characters\n",
    "    # (Keep basic punctuation; strip control chars)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s,.!?'-]\", \" \", text)\n",
    "\n",
    "    # Normalize whitespace and lowercase\n",
    "    text = whitespace_pattern.sub(\" \", text).strip().lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "data[\"text_clean\"] = data.apply(clean_and_deidentify, axis=1)\n",
    "\n",
    "data[[\"IssueKey\", \"Body\", \"text_clean\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f3562",
   "metadata": {},
   "source": [
    "### Quick sanity check\n",
    "\n",
    "Letâ€™s look at a few random comments to ensure:\n",
    "\n",
    "- They are de-identified.  \n",
    "- They still make sense semantically (so an LLM can interpret them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 â€“ Random sample for manual inspection\n",
    "\n",
    "data.sample(min(10, len(data)), random_state=42)[[\"IssueKey\", \"text_clean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fnxrxggtwta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key (REPLACE WITH YOUR ACTUAL KEY!)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-YOUR-NEW-KEY-HERE\"\n",
    "\n",
    "# Run automated theme discovery\n",
    "print(\"ðŸ¤– Starting automated theme analysis...\")\n",
    "print(f\"ðŸ“Š Analyzing {len(data)} comments from {data['IssueKey'].nunique()} issues...\\n\")\n",
    "\n",
    "themes_result = analyze_themes_automated(data, num_themes=10)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISCOVERED THEMES\")\n",
    "print(\"=\"*80)\n",
    "for i, theme in enumerate(themes_result[\"themes\"], 1):\n",
    "    print(f\"\\n{i}. {theme['theme']}\")\n",
    "    print(f\"   {theme['description']}\")\n",
    "    print(f\"   Examples: {', '.join(theme['examples'][:2])}\")\n",
    "    \n",
    "print(f\"\\nâœ… Theme discovery complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fxhus4lspjv",
   "metadata": {},
   "source": [
    "## 7. Automated Theme Discovery with OpenAI\n",
    "\n",
    "Now we'll use the OpenAI API to automatically discover themes instead of manual copying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb4701",
   "metadata": {},
   "source": [
    "## 5. Prepare text for LLM theme discovery\n",
    "\n",
    "Now weâ€™ll format the comments into a prompt-friendly structure you can paste into Copilot / GPT.\n",
    "\n",
    "For example, weâ€™ll generate something like:\n",
    "\n",
    "```text\n",
    "1. [Issue SWIFT-7544] this was not ready for prd by 10/27 ...\n",
    "2. [Issue SWIFT-7543] adding the november release ...\n",
    "...\n",
    "```\n",
    "\n",
    "You can then paste that list into your LLM with the prompt:\n",
    "\n",
    "> â€œHere are Jira comments about why work wasnâ€™t completed in October.  \n",
    "> Read all of them and propose 5â€“10 high-level themes, each with:  \n",
    "> â€¢ A short title  \n",
    "> â€¢ A one-sentence description of the reason for delay  \n",
    "> â€¢ A few example phrases that fit that theme.  \n",
    "> Return JSON with keys: `theme`, `description`, `examples`.â€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 â€“ Build grouped, ordered comments for LLM input\n",
    "\n",
    "rows_for_llm = []\n",
    "\n",
    "# We'll group by IssueKey, and within each group the rows are already sorted by Timestamp\n",
    "for issue, grp in data.groupby(\"IssueKey\", sort=True):\n",
    "    rows_for_llm.append(f\"Issue {issue}:\")\n",
    "    for _, row in grp.iterrows():\n",
    "        ts = row[\"Timestamp\"] if \"Timestamp\" in row and pd.notna(row[\"Timestamp\"]) else None\n",
    "        txt = row[\"text_clean\"]\n",
    "\n",
    "        if ts is not None:\n",
    "            ts_str = ts.strftime(\"%Y-%m-%d %H:%M\")\n",
    "            rows_for_llm.append(f\"  - [{ts_str}] {txt}\")\n",
    "        else:\n",
    "            rows_for_llm.append(f\"  - {txt}\")\n",
    "    rows_for_llm.append(\"\")  # blank line between issues\n",
    "\n",
    "llm_input_block = \"\\n\".join(rows_for_llm)\n",
    "\n",
    "print(llm_input_block)  # show the first ~4000 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f2754",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "\n",
    "1. Copy the printed `llm_input_block` text above (the numbered list of comments).  \n",
    "2. Paste it into your LLM with a prompt like:\n",
    "\n",
    "> â€œHere are Jira comments about why work wasnâ€™t completed in October.  \n",
    "> Read all of them and propose 5â€“10 high-level themes, each with:  \n",
    "> â€¢ A short title  \n",
    "> â€¢ A one-sentence description of the reason for delay  \n",
    "> â€¢ A few example phrases that fit that theme.  \n",
    "> Return JSON with keys: `theme`, `description`, `examples`.â€\n",
    "\n",
    "3. Review and possibly edit the returned themes and descriptions.  \n",
    "4. Decide on your final theme list (e.g. in a separate document or directly in the next cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb916484",
   "metadata": {},
   "source": [
    "## 6. (Optional) Assign themes back to each comment\n",
    "\n",
    "After the LLM proposes themes and you finalize them, you can also ask the LLM to **assign a theme to each comment**.\n",
    "\n",
    "### Example prompt for assignment\n",
    "\n",
    "For each comment (or for small batches), you might say:\n",
    "\n",
    "> â€œHere is a Jira comment about why work wasnâ€™t completed:  \n",
    "> `<comment text>`  \n",
    ">  \n",
    "> Based on the following themes:  \n",
    "> 1. CRQ timing / missed release window  \n",
    "> 2. Waiting on other team / vendor  \n",
    "> 3. Approvals / leadership not finalized  \n",
    "> 4. Requirements unclear / design not complete  \n",
    "> 5. Testing / QA / UAT delays  \n",
    "> 6. Capacity / competing priorities  \n",
    "> 7. Other / unclear  \n",
    ">  \n",
    "> Return only the number and name of the single best-fitting theme.â€\n",
    "\n",
    "You can copy the numbered list of comments and ask the LLM to return a simple table or JSON mapping `comment_number -> theme`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba2612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json\n",
    "\n",
    "with open(\"C:/Users/Andy/Desktop/Text_Analysis/JSON_themes.json\", \"r\") as f:\n",
    "    json_data = json.load(f)  # Changed from 'data' to 'json_data'\n",
    "\n",
    "themes = pd.DataFrame(json_data[\"themes\"])\n",
    "issues = pd.DataFrame(json_data[\"issues\"])\n",
    "\n",
    "summary = {\n",
    "    \"Total Issues\": len(issues),\n",
    "    \"Total Themes\": len(themes),\n",
    "    \"Avg Themes per Issue\": issues[\"themes\"].apply(len).mean()\n",
    "}\n",
    "pd.DataFrame([summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee29bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "json_path = r\"C:\\Users\\Andy\\Desktop\\Text_Analysis\\JSON_themes.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)  # Changed from 'data' to 'json_data'\n",
    "\n",
    "# Adjust these keys if your JSON is shaped differently\n",
    "themes_raw = json_data.get(\"themes\", [])\n",
    "issues_raw = json_data.get(\"issues\", [])\n",
    "\n",
    "themes_df = pd.DataFrame(themes_raw)\n",
    "issues_df = pd.DataFrame(issues_raw)\n",
    "\n",
    "print(\"Themes:\", themes_df.shape)\n",
    "print(\"Issues:\", issues_df.shape)\n",
    "themes_df.head(), issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many issues each theme appears in\n",
    "all_theme_mentions = [t for theme_list in issues_df[\"themes\"] for t in theme_list]\n",
    "theme_counts = Counter(all_theme_mentions)\n",
    "\n",
    "theme_freq = (\n",
    "    pd.DataFrame(theme_counts.items(), columns=[\"Theme\", \"Count\"])\n",
    "    .sort_values(\"Count\", ascending=False)\n",
    ")\n",
    "\n",
    "theme_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.barh(theme_freq[\"Theme\"], theme_freq[\"Count\"])\n",
    "plt.gca().invert_yaxis()  # largest at top\n",
    "\n",
    "plt.title(\"Top Reasons for October Jira Delays (by # of Issues)\")\n",
    "plt.xlabel(\"Number of Issues\")\n",
    "plt.ylabel(\"Theme\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
